class Tokenizer:
    def __init__(self, vocab_size=2048):
        self.vocab_size = vocab_size
        self.token_to_id = {}
        self.id_to_token = {}

    def train(self, text):
        pass

    def encode(self, text):
        pass

    def decode(self, ids):
        pass
